{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-11T18:02:26.461500Z","iopub.execute_input":"2021-07-11T18:02:26.462175Z","iopub.status.idle":"2021-07-11T18:02:26.476062Z","shell.execute_reply.started":"2021-07-11T18:02:26.462121Z","shell.execute_reply":"2021-07-11T18:02:26.474930Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/mlrtugas/02-14-2018.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input/ids-intrusion-csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:02:26.478064Z","iopub.execute_input":"2021-07-11T18:02:26.478377Z","iopub.status.idle":"2021-07-11T18:02:26.486840Z","shell.execute_reply.started":"2021-07-11T18:02:26.478348Z","shell.execute_reply":"2021-07-11T18:02:26.485717Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install sparkmagic\n!pip install pyspark\n#!ls ../input/ids-intrusion-csv -a","metadata":{"execution":{"iopub.status.busy":"2021-07-11T18:02:26.488862Z","iopub.execute_input":"2021-07-11T18:02:26.489192Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f5bad099c50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sparkmagic/\u001b[0m\n\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7f5bad099f90>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sparkmagic/\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col, isnan, when, trim\nfrom pyspark.sql import DataFrame\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pyspark.mllib.classification import SVMWithSGD, SVMModel\n\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom functools import reduce\n\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sqlContext = SQLContext(spark.sparkContext)\ndata1 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-14-2018.csv')\ndata2 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-15-2018.csv')\ndata3 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-16-2018.csv')\ndata4 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-21-2018.csv')\ndata5 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-22-2018.csv')\ndata6 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-23-2018.csv')\ndata7 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-28-2018.csv')\ndata8 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/03-01-2018.csv')\ndata9 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/03-02-2018.csv')\ndata10 = sqlContext.read.format( 'com.databricks.spark.csv' ).options( header='true',inferSchema='true' ).csv( '/kaggle/input/ids-intrusion-csv/02-20-2018.csv')\n\ndata10 = data10.drop('Flow ID')\ndata10 = data10.drop('Src IP')\ndata10 = data10.drop('Src Port')\ndata10 = data10.drop('Dst IP')\n\ndef unionAll(*dfs):\n    return reduce(DataFrame.unionAll,dfs)\ndef to_null(c):\n    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n\ndata = unionAll(data1,data2,data3,data4,data5,data6,data7,data8,data9,data10)\n#data = unionAll(data1)\ndata = data.drop('Timestamp')\ndata = data.filter(data.Label != 'Label')\n#print(data.count())\ndata = data.select([to_null(c).alias(c) for c in data.columns]).na.drop()\n#print(data.count())\n#data.printSchema()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.printSchema()\n#data.select(\"Label\").show()\n#distinct_ids = [x.Label for x in data.select('Label').distinct().collect()]\n#distinct_ids\ncounts = data.groupBy('Label').count().sort('count',ascending=False)\ncounts.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stages = []\nindexer = StringIndexer(inputCol='Label',outputCol='label').fit(data)\nstages += [indexer]\n#df_ind = indexer.transform(data)\n#df_ind.printSchema()\n#df = df_ind.drop('Label')\n#df.printSchema()\n#counts = df.groupBy('Label_index').count().sort('Label_index',ascending=False)\n#counts.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_features = [t[0] for t in data.dtypes if t[1] != 'str']\n#numeric_features\n#df.select(numeric_features).describe().toPandas().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_features.pop()\nnumeric_features\nassembler = VectorAssembler(inputCols=numeric_features,outputCol=\"features\")\nstages += [assembler]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(data)\ndf = pipelineModel.transform(data)\nselectedCols = ['label','features'] + numeric_features\ndf = df.select(selectedCols)\n#df.printSchema()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\nscaled_df = standardScaler.fit(df).transform(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(df.count())\n(train, test) = scaled_df.randomSplit([0.8,0.2], seed=2020)\n#train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(maxIter=3, regParam=0.3, elasticNetParam=0.8)\n\n# Fit the model\nlrModel = lr.fit(train)\n#predictions = lrModel.transform(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = lrModel.transform(test)\n#print(predictions.prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"label\")\nprint(\"F1 Score                        :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})))\nprint(\"Accuracy                        :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})))\n\nprint(\"Weighted Precision              :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})))\nprint(\"Weighted Recall                 :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})))\n\nprint(\"Weighted True Positive Rate     :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"weightedTruePositiveRate\"})))\nprint(\"Weighted False Positive Rate    :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"weightedFalsePositiveRate\"})))\nprint(\"Weighted F Measure              :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"weightedFMeasure\"})))\n\nprint(\"True Positive Rate by Label     :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"truePositiveRateByLabel\"})))\nprint(\"False Positive Rate by Label    :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"falsePositiveRateByLabel\"})))\n\nprint(\"Precision By Label              :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"precisionByLabel\"})))\nprint(\"Recall By Label                 :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"recallByLabel\"})))\nprint(\"F Measure by Label              :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"fMeasureByLabel\"})))\n\nprint(\"Log Loss                        :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"logLoss\"})))\nprint(\"Hamming Loss                    :\" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"hammingLoss\"})))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}